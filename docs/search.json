[
  {
    "objectID": "modelling.html",
    "href": "modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "This page takes the Diabetes dataset and attempts to fit several models to it."
  },
  {
    "objectID": "modelling.html#modelling-file",
    "href": "modelling.html#modelling-file",
    "title": "Modelling",
    "section": "",
    "text": "This page takes the Diabetes dataset and attempts to fit several models to it."
  },
  {
    "objectID": "modelling.html#introduction",
    "href": "modelling.html#introduction",
    "title": "Modelling",
    "section": "Introduction",
    "text": "Introduction\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      1        1         1  40      1      0\n2               0      0        0         0  25      1      0\n3               0      1        1         1  28      0      0\n4               0      1        0         1  27      0      0\n5               0      1        1         1  24      0      0\n6               0      1        1         1  25      1      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       1                 0\n2                    0            1      0       0                 0\n3                    0            0      1       0                 0\n4                    0            1      1       1                 0\n5                    0            1      1       1                 0\n6                    0            1      1       1                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             1           0       5       18       15        1   0   9\n2             0           1       3        0        0        0   0   7\n3             1           1       5       30       30        1   0   9\n4             1           0       2        0        0        0   0  11\n5             1           0       2        3        0        0   0  11\n6             1           0       2        0        2        0   1  10\n  Education Income\n1         4      3\n2         6      1\n3         4      8\n4         3      6\n5         5      4\n6         6      8\n\n\nAbove is the head of the diabetes data. Of note, it is unclear whether the diabetes studied is type 1 or type 2. Previously, we discovered that the heavy consumption of alcohol, entered here as 1 for heavy and 0 for not, has an 85% accuracy for predicting diabetes. From there, we’ll use age, entered in groups with 13 representing the oldest patients, and 1 being the youngest. (The youngest patient is age 18). BMI, or Body Mass Index, will also be used, and as well as a binary for the consumption of enough vegetables.\nWe will be comparing three different model types: Logistic Regression, Decision Trees, and Random Forest. The Logistic Regression model is useful for binary response variables as it allows for a “traditional” approach to regression while also handling the response. The decision tree seeks to model with the best accuracy, literately choosing results which better the prediction. Random Forest is a decision tree on steroids, which also seeks to predict the best model.\nWe will be using log loss as the metric for choosing the best model in each of the three categories. It’s a logarithmic way of keeping track how well a model matches its predictions to actual outcomes. Lower scores are better.\n\nLogit Model\n\n#First split into training and test sets\nset.seed(1701)\nfortrain&lt;- createDataPartition(y = main$Diabetes_binary, p = 0.7, list = F)\ntrain&lt;- main[fortrain,]\ntest&lt;- main[-fortrain,]\n\n#set the control\ncontrol&lt;- trainControl(method = \"repeatedCV\", number = 5, repeats = 1)\n\nWarning: `repeats` has no meaning for this resampling method.\n\n#First model\nlogit1&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age, data = train,\n               method = \"glm\",\n               trControl = control,\n               metric = \"logLoss\")\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nl1pred&lt;- predict(logit1, test)\nconfusionMatrix(test$Diabetes_binary, l1pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Second model\nlogit2&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age + BMI + Veggies, data = train,\n               method = \"glm\",\n               trControl = control,\n               metric = \"logLoss\")\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nl2pred&lt;- predict(logit2, test)\nconfusionMatrix(test$Diabetes_binary, l2pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65016   484\n         1 10251   352\n                                          \n               Accuracy : 0.8589          \n                 95% CI : (0.8564, 0.8614)\n    No Information Rate : 0.989           \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.042           \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8638          \n            Specificity : 0.4211          \n         Pos Pred Value : 0.9926          \n         Neg Pred Value : 0.0332          \n             Prevalence : 0.9890          \n         Detection Rate : 0.8543          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy : 0.6424          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Third model\nlogit3&lt;- train(Diabetes_binary ~ HvyAlcoholConsump, data = train,\n               method = \"glm\",\n               trControl = control,\n               metric = \"logLoss\")\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nl3pred&lt;- predict(logit3, test)\nconfusionMatrix(test$Diabetes_binary, l3pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nIn this case, the simplest model, with the alcohol use as the only predictor, was the best fitting. Moving on now to the decision tree models.\n\n\nDecision Tree Models\n\n#First Model\ndd1&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age, data = train,\n               method = \"rpart\",\n               trControl = control,\n               metric = \"logLoss\",\n            cp = 0.1)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\ndd1pred&lt;- predict(dd1, test)\nconfusionMatrix(test$Diabetes_binary, dd1pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Second Model\ndd2&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age + BMI + Veggies, data = train,\n               method = \"rpart\",\n               trControl = control,\n               metric = \"logLoss\",\n            cp = 0.1)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\ndd2pred&lt;- predict(dd2, test)\nconfusionMatrix(test$Diabetes_binary, dd2pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65284   216\n         1 10412   191\n                                          \n               Accuracy : 0.8603          \n                 95% CI : (0.8579, 0.8628)\n    No Information Rate : 0.9947          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0246          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.86245         \n            Specificity : 0.46929         \n         Pos Pred Value : 0.99670         \n         Neg Pred Value : 0.01801         \n             Prevalence : 0.99465         \n         Detection Rate : 0.85784         \n   Detection Prevalence : 0.86068         \n      Balanced Accuracy : 0.66587         \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Third Model\ndd3&lt;- train(Diabetes_binary ~ HvyAlcoholConsump, data = train,\n               method = \"rpart\",\n               trControl = control,\n               metric = \"logLoss\",\n            cp = 0.1)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\ndd3pred&lt;- predict(dd1, test)\nconfusionMatrix(test$Diabetes_binary, dd3pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nAgain, the simplest model is the best here. Now for Random Forest.\n\n\nRandom Forest\n\n#First Model\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\ncontrolrf&lt;- trainControl(method = \"repeatedCV\", number = 5, repeats = 1, search = \"grid\")\n\nWarning: `repeats` has no meaning for this resampling method.\n\nmtry&lt;- sqrt(ncol(train))\ntunegrid&lt;- expand.grid(.mtry = 1) \nmodel_grid &lt;- expand.grid(\n   mtry = 1                                  \n   ,splitrule = \"gini\"\n   ,min.node.size = 10\n)\nrf1&lt;- train(Diabetes_binary ~ HvyAlcoholConsump, data = train,\n               method = \"ranger\",\n               trControl = controlrf,\n               metric = \"logLoss\",\n            tuneGrid = model_grid)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nrf1pred&lt;- predict(rf1, test)\nconfusionMatrix(test$Diabetes_binary, rf1pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Second Model\nrf2&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age + BMI + Veggies, data = train,\n               method = \"ranger\",\n               trControl = controlrf,\n               metric = \"logLoss\",\n            tuneGrid = model_grid)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nrf2pred&lt;- predict(rf2, test)\nconfusionMatrix(test$Diabetes_binary, rf2pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#Last Model\nrf3&lt;- train(Diabetes_binary ~ HvyAlcoholConsump + Age, data = train,\n               method = \"ranger\",\n               trControl = controlrf,\n               metric = \"logLoss\",\n            tuneGrid = model_grid)\n\nWarning in train.default(x, y, weights = w, ...): The metric \"logLoss\" was not\nin the result set. Accuracy will be used instead.\n\nrf3pred&lt;- predict(rf3, test)\nconfusionMatrix(test$Diabetes_binary, rf3pred)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 65500     0\n         1 10603     0\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8607          \n            Specificity :     NA          \n         Pos Pred Value :     NA          \n         Neg Pred Value :     NA          \n             Prevalence : 1.0000          \n         Detection Rate : 0.8607          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy :     NA          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nOnce again, the simplest model wins out. For all three model types, the best model is simply the one with the first predictor, alcohol usage."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "main&lt;- read.csv(\"C:/Users/James P/Downloads/diabetes_binary_health_indicators_BRFSS2015.csv\")\nhead(main)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      1        1         1  40      1      0\n2               0      0        0         0  25      1      0\n3               0      1        1         1  28      0      0\n4               0      1        0         1  27      0      0\n5               0      1        1         1  24      0      0\n6               0      1        1         1  25      1      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       1                 0\n2                    0            1      0       0                 0\n3                    0            0      1       0                 0\n4                    0            1      1       1                 0\n5                    0            1      1       1                 0\n6                    0            1      1       1                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             1           0       5       18       15        1   0   9\n2             0           1       3        0        0        0   0   7\n3             1           1       5       30       30        1   0   9\n4             1           0       2        0        0        0   0  11\n5             1           0       2        3        0        0   0  11\n6             1           0       2        0        2        0   1  10\n  Education Income\n1         4      3\n2         6      1\n3         4      8\n4         3      6\n5         5      4\n6         6      8\n\n\nAbove is the head of the data set we are working with. This is a diabetes health indicator set. We will be using the binary Diabetes variable as our response. To note, the site from which the data set originates, kaggle, fails to mention whether this is type 1 or type 2 diabetes. The site also mentions that diabetes increases risk of stroke, heart attack, and high blood pressure, so these variables will not be considered as predictors.\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "eda.html#diabetic-data-introduction",
    "href": "eda.html#diabetic-data-introduction",
    "title": "EDA",
    "section": "",
    "text": "main&lt;- read.csv(\"C:/Users/James P/Downloads/diabetes_binary_health_indicators_BRFSS2015.csv\")\nhead(main)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      1        1         1  40      1      0\n2               0      0        0         0  25      1      0\n3               0      1        1         1  28      0      0\n4               0      1        0         1  27      0      0\n5               0      1        1         1  24      0      0\n6               0      1        1         1  25      1      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       1                 0\n2                    0            1      0       0                 0\n3                    0            0      1       0                 0\n4                    0            1      1       1                 0\n5                    0            1      1       1                 0\n6                    0            1      1       1                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             1           0       5       18       15        1   0   9\n2             0           1       3        0        0        0   0   7\n3             1           1       5       30       30        1   0   9\n4             1           0       2        0        0        0   0  11\n5             1           0       2        3        0        0   0  11\n6             1           0       2        0        2        0   1  10\n  Education Income\n1         4      3\n2         6      1\n3         4      8\n4         3      6\n5         5      4\n6         6      8\n\n\nAbove is the head of the data set we are working with. This is a diabetes health indicator set. We will be using the binary Diabetes variable as our response. To note, the site from which the data set originates, kaggle, fails to mention whether this is type 1 or type 2 diabetes. The site also mentions that diabetes increases risk of stroke, heart attack, and high blood pressure, so these variables will not be considered as predictors.\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "eda.html#analysis",
    "href": "eda.html#analysis",
    "title": "EDA",
    "section": "Analysis",
    "text": "Analysis\nFirstly, for the predictors with two levels, I will borrow the confusion matrix from the caret package to check how well they align for the modelling.\n\nconfusionMatrix(main$Diabetes_binary, main$HvyAlcoholConsump)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction      0      1\n         0 204910  13424\n         1  34514    832\n                                          \n               Accuracy : 0.811           \n                 95% CI : (0.8095, 0.8126)\n    No Information Rate : 0.9438          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : -0.0506         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.85585         \n            Specificity : 0.05836         \n         Pos Pred Value : 0.93852         \n         Neg Pred Value : 0.02354         \n             Prevalence : 0.94380         \n         Detection Rate : 0.80775         \n   Detection Prevalence : 0.86067         \n      Balanced Accuracy : 0.45710         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nWe can see for heavy alcohol consumption, with a 1 for consumption and 0 for non-heavy consumption, that there is an 85% accuracy in predicting diabetes.\n\nconfusionMatrix(main$Diabetes_binary, main$Veggies)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction      0      1\n         0  39229 179105\n         1   8610  26736\n                                          \n               Accuracy : 0.26            \n                 95% CI : (0.2583, 0.2617)\n    No Information Rate : 0.8114          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : -0.0212         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.1299          \n         Pos Pred Value : 0.1797          \n         Neg Pred Value : 0.7564          \n             Prevalence : 0.1886          \n         Detection Rate : 0.1546          \n   Detection Prevalence : 0.8607          \n      Balanced Accuracy : 0.4750          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nFor veggies, we can see that most people who eat veggies do not have diabetes. This negative information could be useful for the model.\n\nlibrary(ggplot2)\nggplot(main, aes(x = BMI, color = Diabetes_binary)) + geom_bar()\n\n\n\n\n\n\n\n\nBMI, or Body Mass Index, may not be a great indicator of diabetes, but the tail certainly seems to be more blue than orange.\n\nggplot(main, aes(x = Age, color = Diabetes_binary)) + geom_bar()\n\n\n\n\n\n\n\n\nDiabetes appears to occur in the oldest age groups."
  },
  {
    "objectID": "eda.html#discussion",
    "href": "eda.html#discussion",
    "title": "EDA",
    "section": "Discussion",
    "text": "Discussion\nFor the modelling page, I am going to try to fit models using BMI, Age, Veggies, and Alcohol as predictors for the the response. Since alcohol already fit diabetes so well, hopefully the other three will improve the fit. Since type 2 diabetes takes time to manifest, age should assist in that prediction. Lastly, someone who consumes vegetables will be at less risk, so there will be a “negative” term in the model.\n\nModelling Page"
  }
]